# å´æ©è¾¾æ·±åº¦å­¦ä¹ 

## 0.1 notation

a useful convention would be to take the data associated with different training examples

|                     notation                     |                           meaning                            |
| :----------------------------------------------: | :----------------------------------------------------------: |
|                       $m$                        |               Amount of data/training example                |
|                  $n_{x}$ or $n$                  |                   è¾“å…¥çš„ç‰¹å¾å‘é‡$x$çš„ç»´åº¦                    |
| $(x,y),x \in \mathbb{R}^{n_{x}}, y \in \{0,1\} $ |   ä¸€ä¸ªæ ·æœ¬ï¼Œ$x$æ˜¯$n_{x}$ç»´çš„ç‰¹å¾å‘é‡ï¼Œæ ‡ç­¾$y$å€¼ä¸º$0$æˆ–$1$    |
|     $(x^{(i)},y^{(i)})...(x^{(m)},y^{(m)})$      |                ç¬¬$i$ä¸ªæ ·æœ¬ å’Œ æœ€åä¸€ä¸ªä¸ªæ ·æœ¬                 |
|                 $m = m_{train}$                  |                è®­ç»ƒé›†çš„æ ·æœ¬æ•° #train example                 |
|                  $m = m_{test}$                  |                 æµ‹è¯•é›†çš„æ ·æœ¬æ•° #test example                 |
|       $X = [x^{(1)},x^{(2)},...,x^{(m)}]$        | $X \in \mathbb{R}^{n_{x} \times m}$<br />è¿™æ˜¯ä¸€ä¸ª$m$åˆ—ï¼Œ$n_{x}$è¡Œçš„çŸ©é˜µï¼Œè¡¨ç¤ºæ‰€æœ‰çš„è®­ç»ƒé›†ï¼ˆåˆ—å‘é‡å †å ï¼‰çš„è¾“å…¥å½¢å¼ |
|       $Y = [y^{(1)},y^{(2)},...,y^{(m)}]$        | $X \in \mathbb{R}^{1 \times m}$<br />è¿™æ˜¯ä¸€ä¸ª$m$åˆ—ï¼Œ$1$è¡Œçš„çŸ©é˜µï¼Œè¡¨ç¤ºè¾“å‡ºæ ‡ç­¾ |
|                       w:=                        |                            æ›´æ–°w                             |



## 0.2 What you'll learn

Courses in this sequence (Specializationå¾®ä¸“ä¸š) :

1. Neural Networks and Deep Learning ç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹ 
   1. Week 1 :    Introduction
   2. Week 2 :    Basics of Neural Network programming ç¥ç»ç½‘ç»œç¼–ç¨‹æ¡†æ¶
   3. Week 3 :    One hidden layer Neural Networks å•éšå±‚ç¥ç»ç½‘ç»œ
   4. Week 4 :    Deep Neural Networks å¤šå±‚ç¥ç»ç½‘ç»œ
2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization è¶…å‚æ•°è°ƒæ•´ã€æ­£åˆ™åŒ–ã€ä¼˜åŒ–ç®—æ³•
3. Structuring your Machine Learning project ç»“æ„åŒ–ä½ çš„æœºå™¨å­¦ä¹ å·¥ç¨‹
4. Convolutional Neural Networks å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰
5. Natural Language Processing (NLP) : Building sequence models è‡ªç„¶è¯­è¨€å¤„ç†ï¼šåºåˆ—æ¨¡å‹

## Week 1. Introduction

1. Week 1 :    Introduction
2. Week 2 :    Basics of Neural Network programming ç¥ç»ç½‘ç»œç¼–ç¨‹æ¡†æ¶
3. Week 3 :    One hidden layer Neural Networks å•éšå±‚ç¥ç»ç½‘ç»œ
4. Week 4 :    Deep Neural Networks å¤šå±‚ç¥ç»ç½‘ç»œ

### 1.1 WHAT IS NEURAL NETWORKS

#### ç¥ç»ç½‘ç»œé‡Œé¢çš„ç¥ç»å…ƒ

ç”±å›¾å¯å¾—ï¼Œé€šè¿‡==ç¥ç»å…ƒ==å¯ä»¥ä»è¾“å…¥xåˆ°è¾“å‡ºyã€‚

<img src="pic/1.2_housing-price-prdiction.png" alt="1.2_housing-price-prdiction" style="zoom:33%;" />

### 1.2 Supervised Leaning in Neural Network

ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œç›‘ç£å­¦ä¹ 

#### NN types

<img src="https://vitalflux.com/wp-content/uploads/2021/11/deep-neural-network-examples.png" alt="avatar" style="zoom:33%;" />

Different NN types are used for different problems:

<img src="https://x-wei.github.io/images/Ng_DLMooc_c1wk1/pasted_image002.png" alt="avatar" style="zoom:33%;" />

#### Structured Data and Unstructured Data

| Structured Data                                | Unstructured Data                         |
| ---------------------------------------------- | ----------------------------------------- |
| database                                       | audio/image/text                          |
| each feature/column has a well-defined meaning | no well-defined meaning for pixels/tokens |
| è¡¨æ ¼å‹æ•°æ®                                     | ä¸€äº›æŠ½è±¡çš„æ•°æ®ï¼ˆéŸ³é¢‘ï¼Œå›¾åƒï¼Œæ–‡æœ¬ï¼‰        |

### 1.3  Why is deep learing taking off

- è®¨è®ºæ·±åº¦å­¦ä¹ å´›èµ·åˆ«åçš„ä¸€äº›ä¸»è¦é©±åŠ¨å› ç´ 

<img src="pic/1.4.Scale drives deep learning progress.png" alt="1.4.Scale drives deep learning progress" style="zoom:33%;" />

- ä¼ ç»Ÿå­¦ä¹ ç®—æ³•
  - æ”¯æŒå‘é‡æœº support vector machine
  - é€»è¾‘å›å½’ logistic regression



<img src="https://i-blog.csdnimg.cn/blog_migrate/1a53b2670471705a0b5b880a436a8589.jpeg" alt="avator" style="zoom: 50%;" />

#### Scale drives deep learning progress

- åœ¨å°è®­ç»ƒé›†é˜¶æ®µ
  - å„ç§ç®—æ³•ï¼ˆNN or ä¼ ç»Ÿç®—æ³•ï¼‰ ä¹‹é—´çš„ç›¸å¯¹é¡ºåºå¹¶ä¸æ˜¯å¾ˆæ˜ç¡®
  - æ€§èƒ½å–å†³äºäººçš„skill at hand engineering features
  - âœ…æœ‰äººè®­ç»ƒçš„ä¸€ä¸ªSVMè¡¨ç°å¾—æ¯”ä¸€ä¸ªå¤§å‹ç¥ç»ç½‘ç»œæ›´å¥½
- åœ¨å¤§è®­ç»ƒé›†é˜¶æ®µ
  - å¤§å‹çš„ç¥ç»ç½‘ç»œå ä¸»å¯¼åœ°ä½ dominate the other approaches

#### computation faster

#### new algorithms

e.g. from sigmoid to ReLU, which in turn speeds up computation too

### 1.4 Test

> 1. What does the analogy "Al is the new electricity" refer to?
>
> - Similar to electricity starting about 100 years ago, Al is transforming multiple industries.
>
> - Al is powering personal devices in our homes and offices, similar to electricity.
>
> - Through the "smart grid"ã€æ™ºèƒ½ç”µç½‘ã€‘, Al is delivering a new wave of electricity.
>
> - Al runs on computers and is thus powered by electricity, but it is letting computers do things not possible before.

è¯·æ³¨æ„: å´æ©è¾¾åœ¨è§†é¢‘ä¸­è¡¨è¾¾äº†åŒæ ·çš„è§‚ç‚¹ã€‚

A

> 2. Which of these are reasons for Deep Learning recently taking off? (Check the three options that apply.)
>
> - We have access to a lot more computationalã€è®¡ç®—çš„ã€‘ power.
>
> - Deep learning has resulted in significant improvements in important applications such as online advertising, speech recognition, and image recognition. 
>
> - Neural Networks are a brand new field.
>
> - We have access to a lot more data.

A D ä¸‰ä¸ªç­”æ¡ˆï¼Ÿï¼Ÿ

> 3. Recall this diagram of iterating over different ML ideas. Which of the statements below are true? (Check all that apply.)
>
> ![avator](https://i-blog.csdnimg.cn/blog_migrate/e226ce02218154289daebb17bb1391b2.png)
>
> - Being able to try out ideas quickly allows deep learning engineers to iterate more quickly.
>
> - Faster computation can help speed up how long a team takes to iterate to a good idea.
>
> - It is faster to train on a big dataset than a small dataset.
>
> - Recent progress in deep learning algorithms has allowed us to train good models faster (even without changing the CPU/GPU hardware).

ABD



> 4. When an experienced deep learning engineer works on a new problem, they can usually use insightã€æ´å¯ŸåŠ›ã€‘ from previous problems to train a good model on the first try, without needing to iterate multiple times through different models. True/False?
>
>    - True
>
>    - False

==B==

> Note:  Maybe some experience may help, but nobody can always find the best model or hyperparameters without iterations.(æ³¨ï¼šä¹Ÿè®¸ä¹‹å‰çš„ä¸€äº›ç»éªŒå¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œä½†æ²¡æœ‰äººæ€»æ˜¯å¯ä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹æˆ–è¶…å‚æ•°è€Œæ— éœ€è¿­ä»£å¤šæ¬¡ã€‚)

> 5. Which one of these plots represents a ReLU activation function?
>
> <img src="https://i-blog.csdnimg.cn/blog_migrate/63c4ff873c68c7b6073da76a5069bb40.png" alt="avator" style="zoom:50%;" />
>
> 

> 6. Images for cat recognition is an example of â€œstructuredâ€ data, because it is represented as a structured array in a computer. 
>
>    True/False?

False

> 7. A demographicã€äººå£ç»Ÿè®¡å­¦ã€‘ dataset with statistics on different cities' population, GDP per capita, economic growth is an example of "unstructured" data because it contains data coming from different sources. 
>
>    True/False?

False

> 8. Why is an RNN (Recurrentã€å¾ªç¯ã€‘ Neural Network) used for machine translation, say translating English to French? (Check all that apply.)
>
>    - ==It can be trained as a supervised learning problem.==
>
>    - It is strictly more powerful than a Convolutional Neural Network (CNN).
>
>    - It is applicableã€ä½¿ç”¨çš„ã€‘ when the input/output is a sequence (e.g., a sequence of words).
>
>    - RNNs represent the recurrent process of Idea->Code->Experiment->idea

==A==C

> Note: ==RNN can be trained as a supervised learning problem.==

> 9. In this diagram which we hand-drew in lecture, what do the horizontal axis (x-axis) and vertical axis (y-axis) represent?
>
> ![avator](https://i-blog.csdnimg.cn/blog_migrate/97c9ab3a648b0f9aa1cf6ed1e5277d81.png)

- ï»¿ï»¿x-axis is the amount of data
- ï»¿ï»¿y-axis (vertical axis) is the performance of the

> 10. Assuming the trends described in the previous question's figure are accurate (and hoping you got the axis labels right), which of the following are true? (Check all that apply.)
>
> - Decreasing the size of a neural network generally does not hurt an algorithm's performance, and it may help significantly.
>
> - Increasing the training set size generally does not hurt an algorithm's performance, and it may help significantly.ğŸ˜„
>
> - Decreasing the training set size generally does not hurt an algorithm's performance, and it may help significantly.
>
> - Increasing the size of a neural network generally does not hurt an algorithm's performance, and it may help significantly.ğŸ˜„

## Week 2. Basics of Neural Network programming ç¥ç»ç½‘ç»œç¼–ç¨‹çš„åŸºç¡€çŸ¥è¯†

### 2.1 Binary Classification

**Logistic regression** is an algorithm for binary classification.

In binary classification, our goal is to learn a **classifier**ã€åˆ†ç±»å™¨ã€‘.

- æ„é€ ç¥ç»ç½‘ç»œæ—¶ï¼Œç”¨åˆ—å‘é‡å †å ï¼Œä¼šè®©æ„å»ºè¿‡ç¨‹much easier

### 2.2 Logistic Regression

è¿™æ˜¯ä¸€ä¸ªç”¨åœ¨**è¾“å‡ºyæ ‡ç­¾æ˜¯0æˆ–1çš„ç›‘ç£å­¦ä¹ é—®é¢˜**ä¸Šçš„å­¦ä¹ ç®—æ³•ã€‚

> ä¹Ÿå°±æ˜¯äºŒå…ƒåˆ†ç±»é—®é¢˜

![2.2 Logistic Regression&sigmod function](pic/2.2 Logistic Regression&sigmod function.jpeg)



- ç»™å‡º$X$ï¼Œ æˆ‘ä»¬æƒ³è¦è·å¾—$\hat{y} = P(y=1|x)$ï¼Œå³è¾“å‡ºä¸º$y$çš„æ¦‚ç‡ã€‚

$$
x \in \mathbb{R}^{n_{x}},ä¸” \hat{y} \in [0,1]
$$

- å‚æ•°ï¼š

$$
w \in \mathbb{R}^{n_{x}}, b \in R
$$



- è¾“å‡ºï¼š

å› ä¸º$\hat{y} \in [0,1]$ï¼Œæ‰€ä»¥æ‰è¦==åŠ ä¸Šsigmodå‡½æ•°==ã€‚$w^{T}x + b$çš„å–å€¼èŒƒå›´å¤ªå¤§äº†ï¼Œæ˜¯$(-\infty,+\infty)$
$$
\hat{y} = \sigma(w^{T}x + b), where \, \sigma(z) = \frac{1}{1+e^{-z}}
$$


- ç›®çš„ï¼š

ä¸æ–­åœ°è°ƒæ•´ä¸¤ä¸ªå‚æ•°ï¼Œä½¿å¾—==æŸå¤±å‡½æ•°==æœ€å°ã€‚



### 2.3 Logistic Regression loss function and cost function 

```
!!! note
Lost functionã€æŸå¤±å‡½æ•°ã€‘ æ˜¯åœ¨å•ä¸ªè®­ç»ƒæ ·æœ¬ä¸­å®šä¹‰çš„ï¼Œå®ƒè¡¡é‡äº†åœ¨å•ä¸ªè®­ç»ƒæ ·æœ¬ä¸Šçš„è¡¨ç°ï¼Œæ˜¯ä¸€ä¸ªå‡¸å‡½æ•°
Cost functionã€æˆæœ¬å‡½æ•°ã€‘ æ˜¯é’ˆå¯¹æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„ï¼Œå®ƒè¡¡é‡äº†åœ¨å…¨ä½“è®­ç»ƒæ ·æœ¬ä¸Šçš„è¡¨ç°ï¼Œæ˜¯ä¸€ä¸ªå‡¸å‡½æ•°
```



![2.3 Logistic Regression cost function](pic/2.3 Logistic Regression cost function.jpeg)

**æˆæœ¬å‡½æ•°è¢«å®šä¹‰ä¸ºå¹³å‡å€¼ï¼Œå³1/m çš„æŸå¤±å‡½æ•°ä¹‹å’Œ**

> - ç›®æ ‡
>   - loss function å°½é‡çš„å°
>     - å½“æ ‡ç­¾y = 1ï¼Œæˆ‘ä»¬éœ€è¦ $\hat{y}$å°½é‡çš„å¤§
>     - å½“æ ‡ç­¾y = 0ï¼Œæˆ‘ä»¬éœ€è¦ $\hat{y}$å°½é‡çš„å°
>     - æ­£å¥½ç¬¦åˆæˆ‘ä»¬çš„ç›´è§‰
>   - cost function å°½é‡çš„å°

### 2.4 Gradient Descent

ç”¨æ¢¯åº¦ä¸‹é™æ³•å»è®­ç»ƒæˆ–å­¦ä¹ è®­ç»ƒé›†ä¸Šçš„å‚æ•° $w$ å’Œ $b$

- ä»åˆå§‹ç‚¹å¼€å§‹æœæœ€é™¡çš„ä¸‹å¡æ–¹å‘èµ°ä¸€æ­¥-------å³æ¢¯åº¦ä¸‹é™ä¸€æ­¥-------å³è¿­ä»£ä¸€æ¬¡
  - $w := w - \alpha \frac{d J(w,b)}{dw}$
    - ä»£ç ï¼š$w := w - \alpha dw$
  - $b := b - \alpha \frac{d J(w,b)}{db}$
    - ä»£ç ï¼š$b := b - \alpha db$
  - $\alpha$æ˜¯å­¦ä¹ ç‡ï¼Œå’Œæ›´æ–°çš„æ¯ä¸€æ­¥çš„æ­¥é•¿æœ‰å…³ç³»
- ä¸æ–­ä¸‹é™ï¼Œå¾ˆæœ‰å¸Œæœ›æ”¶æ•›åˆ°å…¨å±€æœ€ä¼˜è§£

![2.4 Gradient Descent](pic/2.4 Gradient Descent.jpeg)



### 2.5 Derivatives

```
!!! note
å¯¹å¾®ç§¯åˆ†å’Œå¯¼æ•°æœ‰ç›´è§‚çš„ç†è§£
```

### 2.6 More derivatives examples

- The derivative of the function just means the slope of the function. The slope of the function can be different at different points on the function
- If you want to look up the derivative of a function, you can filp open your calculus textbook or look at the Wikipedia.

### 2.7 Computation Graph

ç¥ç»ç½‘ç»œè®¡ç®—æ˜¯é€šè¿‡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­è¿‡ç¨‹æ¥å®ç°çš„ã€‚

æˆ‘ä»¬å…ˆé€šè¿‡å‰å‘ä¼ æ’­è®¡ç®—å‡ºç¥ç»ç½‘ç»œçš„è¾“å‡ºï¼Œ

ç„¶åå†é€šè¿‡åå‘ä¼ æ’­è®¡ç®—å‡ºå¯¹åº”çš„æ¢¯åº¦æˆ–å¯¼æ•°ã€‚

### 2.8 Derivatives with a Computation Graph

<img src="pic/2.7 Computation Graph.png" alt="2.7 Computation Graph" style="zoom:33%;" />

- $d var$ means $\frac{dJ}{dvar}$
  - $da$ means $\frac{dJ}{da}$
  - åœ¨ä»£ç ä¸­å°±å†™æˆdvarå°±å¯ä»¥äº†
- ä¸€ä¸ªåå‘ä¼ æ’­è®¡ç®—çš„ä¾‹å­
  - é“¾å¼æ³•åˆ™
  - $\frac{dJ}{du} = \frac{dJ}{dv} \frac{dJ}{du}$

### 2.9 Logistic Regression Gradient descent é€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™æ³•

æ€ä¹ˆé€šè¿‡è®¡ç®—åå¯¼æ•°æ¥å®ç°logisticå›å½’çš„æ¢¯åº¦ä¸‹é™æ³•

åœ¨æœ¬èŠ‚è§†é¢‘ä¸­ï¼Œä½¿ç”¨å¯¼æ•°æµç¨‹å›¾å»å°±ç®—æ¢¯åº¦ã€‚

æ¢¯åº¦ä¸‹é™æ³•åº”ç”¨åˆ°logisticå›å½’çš„ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ä¸Šã€‚

![2.9 Logistic Regression Gradient descent ](pic/2.9 Logistic Regression Gradient descent .jpeg)

### 2.10 Gradient descent on $m$ examples

æ¢¯åº¦ä¸‹é™æ³•åº”ç”¨åˆ°logisticå›å½’çš„==$m$==ä¸ªè®­ç»ƒæ ·æœ¬ä¸Šã€‚

![2.10 Gradient descent on m examples](pic/2.10 Gradient descent on m examples.png)



### 2.11 Vectorization

å‘é‡åŒ–æ˜¯æ¶ˆé™¤ä½ çš„ä»£ç ä¸­æ˜¾å¼forå¾ªç¯è¯­å¥çš„è‰ºæœ¯

the art of getting rid of explicit for loops in your code

#### What is vectorization

![2.11 what is vectorization](pic/2.11 what is vectorization.png)

```
!!! note
åªè¦æœ‰å…¶ä»–å¯èƒ½ï¼ˆä½¿ç”¨numpyå†…ç½®å‡½æ•°ï¼‰ï¼Œå°±ä¸è¦ä½¿ç”¨æ˜¾å¼forå¾ªç¯
Whenever possible, avoid explicit for-loops.
```

### 2.12 More vectorization examples



- dot
  - $ U = np. dot(A,v)$
  - ![2.12 example1-dot](pic/2.12 example1-dot.jpg)
- exp
  - $ U = np. exp(v)$
- log
  - $ U = np. log(v)$
- abs
  - $ U = np. abs(v)$
- maximun
  - $ U = np. maximun(v,0)$
  - æ±‚å‡ºVä¸­æ‰€æœ‰å…ƒç´ å’Œ0ä¹‹é—´ç›¸æ¯”çš„æœ€å¤§å€¼
- square
  - $ U = V**2$
  - Vä¸­æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹
- Logistic regression derivatives
  - æŠŠ2ä¸ªforå¾ªç¯å˜æˆä¸€ä¸ªäº†
  - ![2.12 example2](pic/2.12 example2.jpg)

### 2.13 Vectorizing Logistic Regression

![2.13 Vectorizing Logistic Regression ](/Users/yeezy/yeezyshappybook/docs/notes/ML/pic/2.13 Vectorizing Logistic Regression .jpg)

```
Z = np.dot(w.T,X) + b
# b æœ¬æ¥æ˜¯ä¸€ä¸ªï¼ˆ1ï¼Œ1ï¼‰çš„çŸ©é˜µï¼Œä¹Ÿå°±æ˜¯ä¸€ä¸ªå®æ•°
# ä½†æ˜¯é€šè¿‡å¹¿æ’­æœºåˆ¶ï¼Œä¼šæ‰©å±•æˆä¸€ä¸ªï¼ˆ1ï¼Œmï¼‰çš„è¡Œå‘é‡
```

### 2.14 Vectorizing Logistic Regression's Gradient Computaion å‘é‡åŒ–é€»è¾‘å›å½’çš„æ¢¯åº¦è¾“å‡º

- ä¸€æ¬¡è¿­ä»£

  - å‰å‘ä¼ æ’­
    - è®¡ç®—å‡º$z$å’Œ$a$

  - åå‘ä¼ æ’­
    - è®¡ç®—å‡º$da$å’Œ$dz$
      - $da=-\frac{y}{b}+\frac{1-y}{1-a}$
      - $dz = a -y$
    - è®¡ç®—å‡º$dw$å’Œ$db$
      - $dw = xdz$
      - $db = dz$
  - æ›´æ–°
    - æ›´æ–°$w$å’Œ$b$
  
  > æ³¨æ„âš ï¸
  >
  > è¿™é‡Œå–$x$æœ‰ä¸¤ä¸ªç‰¹å¾$x_{1}$å’Œ$x_{2}$ï¼Œåˆ†åˆ«æœ‰ä¸¤ä¸ªæƒé‡å‚æ•°$w_{1}$å’Œ$w_{2}$

<img src="pic/2.14 Vectorizing Logistic Regression's Gradient Computaion1.jpg" alt="2.14 Vectorizing Logistic Regression's Gradient Computaion1" style="zoom:33%;" />

<img src="pic/2.14 Vectorizing Logistic Regression's Gradient Computaion2.jpg" alt="2.14 Vectorizing Logistic Regression's Gradient Computaion2" style="zoom:33%;" />

### 2.15 Broadcasting in Python

æ³¨æ„åªæœ‰æŒ‰å…ƒç´ è¿ç®—èƒ½å¹¿æ’­ï¼ŒçŸ©é˜µä¹˜æ³•ç»´åº¦ä¸ä¸€æ ·ä¼šç›´æ¥æŠ¥é”™ï¼

æ›´å¤šï¼Œåœ¨numpyæ–‡æ¡£ä¸­æœç´¢broadcasting

<img src="/2.15 Broadcasting in py.jpg" alt="2.15 Broadcasting in py" style="zoom:50%;" />

### 2.16 A note on py/numpy ectors

<img src="/2.16 A note on py:numpy vectors.jpg" alt="2.16 A note on py:numpy vectors" style="zoom:33%;" />

### 2.17 Quick tour of Jupyter/ python notebooks

### 2.18 Explanation of logistic regression cost function(Optional)

> è¯´æ˜logisticå›å½’æˆæœ¬å‡½æ•°çš„è¡¨è¾¾å¼ä¸ºä»€ä¹ˆæ˜¯è¿™æ ·çš„
>
> for why we like to use that cost function for logitic regression

![2.18 Explanation of logistic regression cost function](pic/2.18 Explanation of logistic regression cost function.jpg)

## Week 3 :    One hidden layer Neural Networks å•éšå±‚ç¥ç»ç½‘ç»œ

### 3.1 Neural Networks Overview

| notation  | meaning                              |
| :-------: | ------------------------------------ |
| ä¸Šæ ‡$[i]$ | ç¥ç»ç½‘ç»œä¸­çš„ç¬¬$i$å±‚ï¼ˆç¬¬0å±‚ä¸ºè¾“å…¥å±‚ï¼‰ |
| ä¸Šæ ‡${i}$ | è¡¨ç¤ºç¬¬$i$ä¸ªè®­ç»ƒæ ·æœ¬                  |

### 3.2 Neural Network Representation ç¥ç»ç½‘ç»œçš„è¡¨ç¤º

