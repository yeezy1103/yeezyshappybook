# AndrewNg-DL-Course1-Week1

1. Week 1 :    Introduction
2. Week 2 :    Basics of Neural Network programming ç¥ç»ç½‘ç»œç¼–ç¨‹æ¡†æ¶
3. Week 3 :    One hidden layer Neural Networks å•éšå±‚ç¥ç»ç½‘ç»œ
4. Week 4 :    Deep Neural Networks å¤šå±‚ç¥ç»ç½‘ç»œ

## 1.1 WHAT IS NEURAL NETWORKS

#### ç¥ç»ç½‘ç»œé‡Œé¢çš„ç¥ç»å…ƒ

ç”±å›¾å¯å¾—ï¼Œé€šè¿‡==ç¥ç»å…ƒ==å¯ä»¥ä»è¾“å…¥xåˆ°è¾“å‡ºyã€‚

<img src="/Users/yeezy/yeezyshappybook/docs/notes/ML/pic/1.2_housing-price-prdiction.png" alt="1.2_housing-price-prdiction" style="zoom:33%;" />

## 1.2 Supervised Leaning in Neural Network

ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œç›‘ç£å­¦ä¹ 

### NN types

![1.1.1](pic/1.1.1.png)Different NN types are used for different problems:

<img src="https://x-wei.github.io/images/Ng_DLMooc_c1wk1/pasted_image002.png" alt="avatar" style="zoom:33%;" />

### Structured Data and Unstructured Data

| Structured Data                                | Unstructured Data                         |
| ---------------------------------------------- | ----------------------------------------- |
| database                                       | audio/image/text                          |
| each feature/column has a well-defined meaning | no well-defined meaning for pixels/tokens |
| è¡¨æ ¼å‹æ•°æ®                                     | ä¸€äº›æŠ½è±¡çš„æ•°æ®ï¼ˆéŸ³é¢‘ï¼Œå›¾åƒï¼Œæ–‡æœ¬ï¼‰        |

## 1.3  Why is deep learing taking off

- è®¨è®ºæ·±åº¦å­¦ä¹ å´›èµ·åˆ«åçš„ä¸€äº›ä¸»è¦é©±åŠ¨å› ç´ 

<img src="/Users/yeezy/yeezyshappybook/docs/notes/ML/pic/1.4.Scale drives deep learning progress.png" alt="1.4.Scale drives deep learning progress" style="zoom:33%;" />

- ä¼ ç»Ÿå­¦ä¹ ç®—æ³•
  - æ”¯æŒå‘é‡æœº support vector machine
  - é€»è¾‘å›å½’ logistic regression



<img src="https://i-blog.csdnimg.cn/blog_migrate/1a53b2670471705a0b5b880a436a8589.jpeg" alt="avator" style="zoom: 50%;" />

### Scale drives deep learning progress

- åœ¨å°è®­ç»ƒé›†é˜¶æ®µ
  - å„ç§ç®—æ³•ï¼ˆNN or ä¼ ç»Ÿç®—æ³•ï¼‰ ä¹‹é—´çš„ç›¸å¯¹é¡ºåºå¹¶ä¸æ˜¯å¾ˆæ˜ç¡®
  - æ€§èƒ½å–å†³äºäººçš„skill at hand engineering features
  - âœ…æœ‰äººè®­ç»ƒçš„ä¸€ä¸ªSVMè¡¨ç°å¾—æ¯”ä¸€ä¸ªå¤§å‹ç¥ç»ç½‘ç»œæ›´å¥½
- åœ¨å¤§è®­ç»ƒé›†é˜¶æ®µ
  - å¤§å‹çš„ç¥ç»ç½‘ç»œå ä¸»å¯¼åœ°ä½ dominate the other approaches

### computation faster

### new algorithms

e.g. from sigmoid to ReLU, which in turn speeds up computation too

## 1.4 Test

> 1. What does the analogy "Al is the new electricity" refer to?
>
> - Similar to electricity starting about 100 years ago, Al is transforming multiple industries.
>
> - Al is powering personal devices in our homes and offices, similar to electricity.
>
> - Through the "smart grid"ã€æ™ºèƒ½ç”µç½‘ã€‘, Al is delivering a new wave of electricity.
>
> - Al runs on computers and is thus powered by electricity, but it is letting computers do things not possible before.

è¯·æ³¨æ„: å´æ©è¾¾åœ¨è§†é¢‘ä¸­è¡¨è¾¾äº†åŒæ ·çš„è§‚ç‚¹ã€‚

A

> 2. Which of these are reasons for Deep Learning recently taking off? (Check the three options that apply.)
>
> - We have access to a lot more computationalã€è®¡ç®—çš„ã€‘ power.
>
> - Deep learning has resulted in significant improvements in important applications such as online advertising, speech recognition, and image recognition. 
>
> - Neural Networks are a brand new field.
>
> - We have access to a lot more data.

A D ä¸‰ä¸ªç­”æ¡ˆï¼Ÿï¼Ÿ

> 3. Recall this diagram of iterating over different ML ideas. Which of the statements below are true? (Check all that apply.)
>
> ![avator](https://i-blog.csdnimg.cn/blog_migrate/e226ce02218154289daebb17bb1391b2.png)
>
> - Being able to try out ideas quickly allows deep learning engineers to iterate more quickly.
>
> - Faster computation can help speed up how long a team takes to iterate to a good idea.
>
> - It is faster to train on a big dataset than a small dataset.
>
> - Recent progress in deep learning algorithms has allowed us to train good models faster (even without changing the CPU/GPU hardware).

ABD



> 4. When an experienced deep learning engineer works on a new problem, they can usually use insightã€æ´å¯ŸåŠ›ã€‘ from previous problems to train a good model on the first try, without needing to iterate multiple times through different models. True/False?
>
>    - True
>
>    - False

==B==

> Note:  Maybe some experience may help, but nobody can always find the best model or hyperparameters without iterations.(æ³¨ï¼šä¹Ÿè®¸ä¹‹å‰çš„ä¸€äº›ç»éªŒå¯èƒ½ä¼šæœ‰æ‰€å¸®åŠ©ï¼Œä½†æ²¡æœ‰äººæ€»æ˜¯å¯ä»¥æ‰¾åˆ°æœ€ä½³æ¨¡å‹æˆ–è¶…å‚æ•°è€Œæ— éœ€è¿­ä»£å¤šæ¬¡ã€‚)

> 5. Which one of these plots represents a ReLU activation function?
>
> <img src="https://i-blog.csdnimg.cn/blog_migrate/63c4ff873c68c7b6073da76a5069bb40.png" alt="avator" style="zoom:50%;" />
>
> 

> 6. Images for cat recognition is an example of â€œstructuredâ€ data, because it is represented as a structured array in a computer. 
>
>    True/False?

False

> 7. A demographicã€äººå£ç»Ÿè®¡å­¦ã€‘ dataset with statistics on different cities' population, GDP per capita, economic growth is an example of "unstructured" data because it contains data coming from different sources. 
>
>    True/False?

False

> 8. Why is an RNN (Recurrentã€å¾ªç¯ã€‘ Neural Network) used for machine translation, say translating English to French? (Check all that apply.)
>
>    - ==It can be trained as a supervised learning problem.==
>
>    - It is strictly more powerful than a Convolutional Neural Network (CNN).
>
>    - It is applicableã€ä½¿ç”¨çš„ã€‘ when the input/output is a sequence (e.g., a sequence of words).
>
>    - RNNs represent the recurrent process of Idea->Code->Experiment->idea

==A==C

> Note: ==RNN can be trained as a supervised learning problem.==

> 9. In this diagram which we hand-drew in lecture, what do the horizontal axis (x-axis) and vertical axis (y-axis) represent?
>
> ![avator](https://i-blog.csdnimg.cn/blog_migrate/97c9ab3a648b0f9aa1cf6ed1e5277d81.png)

- ï»¿ï»¿x-axis is the amount of data
- ï»¿ï»¿y-axis (vertical axis) is the performance of the

> 10. Assuming the trends described in the previous question's figure are accurate (and hoping you got the axis labels right), which of the following are true? (Check all that apply.)
>
> - Decreasing the size of a neural network generally does not hurt an algorithm's performance, and it may help significantly.
>
> - Increasing the training set size generally does not hurt an algorithm's performance, and it may help significantly.ğŸ˜„
>
> - Decreasing the training set size generally does not hurt an algorithm's performance, and it may help significantly.
>
> - Increasing the size of a neural network generally does not hurt an algorithm's performance, and it may help significantly.ğŸ˜„